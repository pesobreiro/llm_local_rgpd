# ğŸ¤– LLM Local RGPD

> **Sistema de IA Local conforme RGPD para Consultadoria e Ensino Superior**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![RGPD Compliant](https://img.shields.io/badge/RGPD-Compliant-green.svg)](./docs/RGPD.md)
[![GPU: RTX 2060](https://img.shields.io/badge/GPU-RTX%202060%206GB-blue.svg)](./docs/hardware.md)

---

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [PorquÃª Local?](#-porquÃª-local)
- [Hardware Suportado](#-hardware-suportado)
- [InstalaÃ§Ã£o RÃ¡pida](#-instalaÃ§Ã£o-rÃ¡pida)
- [Modelos Aprovados](#-modelos-aprovados-rgpd)
- [ConfiguraÃ§Ãµes](#-configuraÃ§Ãµes)
- [DocumentaÃ§Ã£o RGPD](#-documentaÃ§Ã£o-rgpd)
- [Uso](#-uso)

---

## ğŸ¯ VisÃ£o Geral

Este projeto fornece um **sistema completo para execuÃ§Ã£o local de Large Language Models (LLMs)** em conformidade com o Regulamento Geral de ProteÃ§Ã£o de Dados (RGPD/GDPR).

### Casos de Uso

- ğŸ“Š **Consultadoria empresarial** com dados sensÃ­veis (M&A, due diligence)
- ğŸ“ **Ensino superior** com informaÃ§Ã£o de alunos
- ğŸ¥ **Setor da saÃºde** com dados clÃ­nicos
- ğŸ¦ **InstituiÃ§Ãµes financeiras** com dados bancÃ¡rios
- ğŸ›ï¸ **AdministraÃ§Ã£o pÃºblica** com dados pessoais

### PrincÃ­pios Fundamentais

```
ğŸ”’ Dados NUNCA saem do teu computador
ğŸ”’ Zero chamadas Ã  API de terceiros  
ğŸ”’ Controlo total sobre o processamento
ğŸ”’ Conformidade total com RGPD/AI Act
```

---

## ğŸ›¡ï¸ PorquÃª Local?

### Problema: LLMs na Cloud = Risco RGPD

| Aspecto | Cloud (OpenAI, Claude, etc.) | Local (Este Projeto) |
|---------|------------------------------|----------------------|
| **Dados** | Enviados para servidores externos | Ficam no teu disco |
| **Processamento** | Nos EUA ou outras jurisdiÃ§Ãµes | No teu hardware |
| **RetenÃ§Ã£o** | PolÃ­ticas opacas do fornecedor | Tu decides |
| **Auditoria** | ImpossÃ­vel verificar | Totalmente transparente |
| **SubcontrataÃ§Ã£o** | Qualificada como "subcontratante" | NÃ£o aplica |
| **Consentimento** | NecessÃ¡rio informar destino | Simplificado |

### FundamentaÃ§Ã£o JurÃ­dica

> **Artigo 28Âº do RGPD** - Tratamento por conta do responsÃ¡vel
> 
> Ao usar LLMs na cloud, o fornecedor qualifica-se como "subcontratante", exigindo:
> - Contrato de subcontrataÃ§Ã£o formal
> - AvaliaÃ§Ã£o de impacto Ã  proteÃ§Ã£o de dados (AIPD)
> - Garantias de transferÃªncia internacional (CCT)
> 
> **SoluÃ§Ã£o:** Processamento local elimina a figura de subcontratante.

---

## ğŸ’» Hardware Suportado

### ConfiguraÃ§Ã£o MÃ­nima

```yaml
CPU: 4 cores (8 threads)
RAM: 16 GB
GPU: 4 GB VRAM (opcional mas recomendado)
Disco: 50 GB livres (SSD recomendado)
Sistema: Linux (Ubuntu 22.04+), macOS, Windows WSL2
```

### ConfiguraÃ§Ã£o Recomendada

```yaml
CPU: 8+ cores (16+ threads)
RAM: 32 GB
GPU: 8+ GB VRAM (RTX 3060, RTX 4060, etc.)
Disco: 200 GB SSD NVMe
Sistema: Linux Ubuntu 24.04 LTS
```

### Hardware de Desenvolvimento

Este projeto foi testado e otimizado para:

```
MSI GL65 Leopard
â”œâ”€â”€ CPU: Intel i7-10875H (8C/16T)
â”œâ”€â”€ RAM: 32 GB DDR4
â”œâ”€â”€ GPU: NVIDIA RTX 2060 Mobile (6GB VRAM)
â”œâ”€â”€ Disco: NVMe 512GB
â””â”€â”€ OS: Ubuntu 24.04 LTS
```

---

## âš¡ InstalaÃ§Ã£o RÃ¡pida

```bash
# 1. Clonar o repositÃ³rio
git clone https://github.com/[USER]/llm_local_rgpd.git
cd llm_local_rgpd

# 2. Executar script de instalaÃ§Ã£o automatizada
chmod +x scripts/install.sh
./scripts/install.sh

# 3. Verificar instalaÃ§Ã£o
ollama --version
nvidia-smi

# 4. Baixar modelos aprovados
ollama pull llama3.2:8b
ollama pull mistral:7b
ollama pull qwen2.5-coder:7b
```

ğŸ“– Ver [guia detalhado de instalaÃ§Ã£o](./docs/instalacao.md)

---

## âœ… Modelos Aprovados RGPD

### Modelos Locais Recomendados (6GB VRAM)

| Modelo | Tamanho | Uso Ideal | LicenÃ§a | VRAM |
|--------|---------|-----------|---------|------|
| **Llama 3.2 8B** | 8B params | Geral, apresentaÃ§Ãµes | Llama 3.2 | ~5GB |
| **Mistral 7B** | 7B params | AnÃ¡lise, PT-PT excelente | Apache 2.0 | ~4.5GB |
| **Qwen 2.5 7B** | 7B params | CÃ³digo, STEM | Apache 2.0 | ~4.5GB |
| **Gemma 2 9B** | 9B params | Pesquisa acadÃ©mica | Gemma | ~5.5GB |
| **Phi-4 14B** | 14B params | RaciocÃ­nio complexo | MIT | ~7GB* |

\* *Phi-4 14B requer CPU offload parcial com 6GB VRAM*

### Modelos com CPU Offload (32GB RAM necessÃ¡rios)

| Modelo | Tamanho | Uso | EstratÃ©gia |
|--------|---------|-----|------------|
| **Llama 3.3 70B** | 70B | AnÃ¡lise profunda | GPU: 6GB + RAM: 20GB |
| **Qwen 2.5 32B** | 32B | Especialista | GPU: 6GB + RAM: 15GB |
| **Mixtral 8x7B** | 47B | Tarefas diversas | GPU: 6GB + RAM: 25GB |

### âš ï¸ Modelos a EVITAR (Cloud-only)

- âŒ GPT-4, GPT-3.5 (OpenAI) - Cloud obrigatÃ³ria
- âŒ Claude (Anthropic) - Cloud obrigatÃ³ria
- âŒ Gemini Pro (Google) - Cloud obrigatÃ³ria
- âŒ Mistral Large 2 (123B) - Requer 62GB+ VRAM

---

## âš™ï¸ ConfiguraÃ§Ãµes

### Otimizado para RTX 2060 6GB

Ver [`configs/rtx2060_6gb.conf`](./configs/rtx2060_6gb.conf)

```bash
# Copiar configuraÃ§Ã£o otimizada
cp configs/rtx2060_6gb.conf ~/.ollama/config.toml

# Reiniciar serviÃ§o
sudo systemctl restart ollama
```

### VariÃ¡veis de Ambiente

```bash
# Adicionar ao ~/.bashrc ou ~/.zshrc

# Limitar VRAM usada (deixa margem para sistema)
export OLLAMA_GPU_OVERHEAD=512MB

# NÃºmero de requisiÃ§Ãµes paralelas
export OLLAMA_NUM_PARALLEL=1

# MÃ¡ximo de modelos carregados simultaneamente
export OLLAMA_MAX_LOADED_MODELS=1

# Visible CUDA devices
export CUDA_VISIBLE_DEVICES=0
```

---

## ğŸ“š DocumentaÃ§Ã£o RGPD

### Guias DisponÃ­veis

- [ğŸ“– Fundamentos RGPD + IA](./docs/RGPD.md)
- [ğŸ”’ Checklist de Conformidade](./docs/checklist_rgpd.md)
- [ğŸ“‹ Registo de Atividades de Tratamento (RAT)](./templates/rat_llm_local.md)
- [âš–ï¸ AvaliaÃ§Ã£o de Impacto Ã  ProteÃ§Ã£o de Dados (AIPD)](./templates/aipd_template.md)
- [ğŸ“ Contrato de SubcontrataÃ§Ã£o (para casos hÃ­bridos)](./templates/contrato_subcontratacao.md)

### DecisÃµes RGPD Documentadas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECISÃƒO: Uso de LLM Local vs Cloud                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data: 2025-02-26                                       â”‚
â”‚  ResponsÃ¡vel: [Nome do ResponsÃ¡vel pelo Tratamento]     â”‚
â”‚  FundamentaÃ§Ã£o:                                         â”‚
â”‚    âœ“ Art. 28Âº RGPD - NÃ£o hÃ¡ subcontrataÃ§Ã£o             â”‚
â”‚    âœ“ Art. 32Âº RGPD - Medidas tÃ©cnicas implementadas    â”‚
â”‚    âœ“ Art. 25Âº RGPD - Privacy by Design                 â”‚
â”‚  Modelo escolhido: Processamento 100% local             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Uso

### Interface de Linha de Comandos

```bash
# Modo interativo
ollama run llama3.2:8b

# Comando Ãºnico
echo "Resume este contrato: $(cat contrato.txt)" | ollama run llama3.2:8b

# API local (para integraÃ§Ã£o)
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:8b",
  "prompt": "Analise este contrato de trabalho...",
  "stream": false
}'
```

### IntegraÃ§Ã£o com Python

```python
import requests

# Consulta totalmente local
response = requests.post('http://localhost:11434/api/generate', json={
    'model': 'llama3.2:8b',
    'prompt': 'Analise este texto: ...',
    'options': {
        'temperature': 0.7,
        'num_ctx': 8192
    }
})

result = response.json()
print(result['response'])
```

### Interface Web (Open WebUI)

```bash
# Instalar interface web opcional
docker run -d -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main
```

Aceder em: `http://localhost:3000`

---

## ğŸ“Š Benchmarks no Hardware de ReferÃªncia

| Modelo | Tokens/seg | LatÃªncia (TTFT) | Uso VRAM | Qualidade |
|--------|-----------|-----------------|----------|-----------|
| Llama 3.2 8B Q4 | 28-35 | 1.2s | 5.0GB | â­â­â­â­â­ |
| Mistral 7B Q4 | 25-32 | 1.1s | 4.5GB | â­â­â­â­â­ |
| Qwen 2.5 7B Q4 | 26-33 | 1.0s | 4.5GB | â­â­â­â­â˜† |
| Gemma 2 9B Q4 | 20-28 | 1.5s | 5.5GB | â­â­â­â­â˜† |

*Testado em MSI GL65 (i7-10875H, RTX 2060 6GB, 32GB RAM)*

---

## ğŸ¤ ContribuiÃ§Ã£o

```bash
# Fazer fork do projeto
# Criar branch
git checkout -b feature/nova-feature

# Commit
git commit -m "feat: adicionar suporte para [modelo]"

# Push
git push origin feature/nova-feature

# Abrir Pull Request
```

---

## ğŸ“œ LicenÃ§a

Este projeto Ã© licenciado sob MIT License - ver [LICENSE](LICENSE) para detalhes.

**Nota importante:** Os modelos LLM tÃªm as suas prÃ³prias licenÃ§as (Apache 2.0, Llama, etc.). Verificar compatibilidade com o uso pretendido.

---

## âš ï¸ Disclaimer Legal

> Este projeto fornece **ferramentas tÃ©cnicas** para conformidade RGPD, mas **nÃ£o constitui aconselhamento jurÃ­dico**. 
>
> Para implementaÃ§Ã£o em contexto empresarial, consultar um **Encarregado de ProteÃ§Ã£o de Dados (EPD)** ou advogado especializado em proteÃ§Ã£o de dados.

---

## ğŸ“ Suporte

- ğŸ“§ Email: [teu-email]
- ğŸ’¬ Issues: [GitHub Issues]
- ğŸ“– Wiki: [GitHub Wiki]

---

<p align="center">
  <strong>ğŸ›¡ï¸ Protege os dados dos teus clientes. MantÃ©m o controlo. Cumpre o RGPD.</strong>
</p>
