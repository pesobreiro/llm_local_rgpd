# ðŸ“¥ Guia de InstalaÃ§Ã£o Detalhado

Este guia explica passo a passo a instalaÃ§Ã£o do sistema LLM Local RGPD.

---

## Ãndice

1. [PrÃ©-requisitos](#1-prÃ©-requisitos)
2. [InstalaÃ§Ã£o Automatizada](#2-instalaÃ§Ã£o-automatizada-recomendada)
3. [InstalaÃ§Ã£o Manual](#3-instalaÃ§Ã£o-manual)
4. [ConfiguraÃ§Ã£o PÃ³s-InstalaÃ§Ã£o](#4-configuraÃ§Ã£o-pÃ³s-instalaÃ§Ã£o)
5. [InstalaÃ§Ã£o de Modelos](#5-instalaÃ§Ã£o-de-modelos)
6. [VerificaÃ§Ã£o](#6-verificaÃ§Ã£o)
7. [ResoluÃ§Ã£o de Problemas](#7-resoluÃ§Ã£o-de-problemas)

---

## 1. PrÃ©-requisitos

### 1.1 Hardware MÃ­nimo

```yaml
CPU: 4 cores (8 threads) - Intel i5/AMD Ryzen 5 ou superior
RAM: 16 GB DDR4
Disco: 50 GB livres em SSD
GPU: Opcional (4GB+ VRAM recomendado)
```

### 1.2 Hardware Recomendado (Testado)

```yaml
CPU: Intel i7-10875H (8C/16T) ou equivalente
RAM: 32 GB DDR4
Disco: 200 GB SSD NVMe
GPU: NVIDIA RTX 2060 (6GB VRAM) ou superior
```

### 1.3 Software NecessÃ¡rio

- **Sistema Operativo:** Ubuntu 22.04 LTS ou 24.04 LTS
- **Kernel:** 5.15+ (com suporte para drivers NVIDIA recentes)
- **Internet:** ConexÃ£o para download inicial (posteriormente funciona offline)

### 1.4 VerificaÃ§Ã£o PrÃ©via

Execute estes comandos para verificar o sistema:

```bash
# Verificar versÃ£o do Ubuntu
lsb_release -a

# Verificar CPU
lscpu | grep "Model name"

# Verificar RAM
free -h

# Verificar espaÃ§o em disco
df -h

# Verificar GPU (se NVIDIA)
nvidia-smi
```

---

## 2. InstalaÃ§Ã£o Automatizada (Recomendada)

### 2.1 Download do Projeto

```bash
# Criar diretÃ³rio git se nÃ£o existir
mkdir -p ~/git
cd ~/git

# Clonar o repositÃ³rio
git clone https://github.com/[USER]/llm_local_rgpd.git
cd llm_local_rgpd
```

### 2.2 Executar Script de InstalaÃ§Ã£o

```bash
# Dar permissÃµes de execuÃ§Ã£o
chmod +x scripts/install.sh

# Executar instalaÃ§Ã£o (modo GPU NVIDIA)
./scripts/install.sh --gpu nvidia --models minimal

# Ou para instalaÃ§Ã£o CPU apenas
./scripts/install.sh --gpu cpu --models minimal

# Ou instalaÃ§Ã£o completa (todos os modelos)
./scripts/install.sh --gpu nvidia --models all
```

### 2.3 O que o Script Faz

1. âœ… Atualiza repositÃ³rios do sistema
2. âœ… Instala dependÃªncias (curl, git, docker, etc.)
3. âœ… Instala CUDA Toolkit (se GPU NVIDIA)
4. âœ… Instala Ollama
5. âœ… Configura Ollama para hardware especÃ­fico
6. âœ… Baixa modelos selecionados
7. âœ… Cria modelo personalizado PT-PT
8. âœ… Configura aliases e helpers
9. âœ… Executa testes de verificaÃ§Ã£o

### 2.4 Tempo de InstalaÃ§Ã£o

| Componente | Tempo Estimado |
|------------|---------------|
| DependÃªncias do sistema | 5-10 minutos |
| CUDA Toolkit | 10-15 minutos |
| Ollama | 2-3 minutos |
| Modelo Llama 3.2 8B | 5-10 minutos (download) |
| Modelo Mistral 7B | 4-8 minutos (download) |
| ConfiguraÃ§Ã£o | 2-3 minutos |
| **Total** | **30-50 minutos** |

---

## 3. InstalaÃ§Ã£o Manual

Se preferires controlo total ou o script automatizado falhar.

### 3.1 Instalar DependÃªncias

```bash
sudo apt update
sudo apt install -y curl wget git software-properties-common \
    apt-transport-https ca-certificates gnupg lsb-release jq htop
```

### 3.2 Instalar Drivers NVIDIA (se aplicÃ¡vel)

```bash
# Verificar se drivers jÃ¡ estÃ£o instalados
nvidia-smi

# Se nÃ£o, instalar
sudo apt install -y ubuntu-drivers-common
sudo ubuntu-drivers autoinstall

# Reiniciar apÃ³s instalaÃ§Ã£o
sudo reboot
```

### 3.3 Instalar CUDA Toolkit (Opcional mas Recomendado)

```bash
# Ubuntu 24.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install -y cuda-toolkit-12-6

# Adicionar ao PATH
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

### 3.4 Instalar Ollama

```bash
# MÃ©todo oficial (recomendado)
curl -fsSL https://ollama.com/install.sh | sh

# Verificar instalaÃ§Ã£o
ollama --version
```

### 3.5 Configurar Ollama para RTX 2060 6GB

```bash
# Criar diretÃ³rio de override do systemd
sudo mkdir -p /etc/systemd/system/ollama.service.d

# Criar ficheiro de configuraÃ§Ã£o
sudo tee /etc/systemd/system/ollama.service.d/override.conf > /dev/null <<EOF
[Service]
Environment="CUDA_VISIBLE_DEVICES=0"
Environment="OLLAMA_GPU_OVERHEAD=512MB"
Environment="OLLAMA_NUM_PARALLEL=1"
Environment="OLLAMA_MAX_LOADED_MODELS=1"
Environment="OLLAMA_KEEP_ALIVE=30m"
EOF

# Recarregar e reiniciar
sudo systemctl daemon-reload
sudo systemctl restart ollama
sudo systemctl enable ollama

# Verificar estado
sudo systemctl status ollama --no-pager
```

---

## 4. ConfiguraÃ§Ã£o PÃ³s-InstalaÃ§Ã£o

### 4.1 Configurar VariÃ¡veis de Ambiente

Adicionar ao `~/.bashrc`:

```bash
# Ollama settings
export OLLAMA_GPU_OVERHEAD=512MB
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_MAX_LOADED_MODELS=1

# CUDA settings
export CUDA_VISIBLE_DEVICES=0

# Aliases Ãºteis
alias llm='ollama run llm-local-pt'
alias llm-list='ollama list'
alias llm-ps='ollama ps'
alias gpu='nvidia-smi'
alias gpu-watch='watch -n 1 nvidia-smi'
```

Aplicar alteraÃ§Ãµes:
```bash
source ~/.bashrc
```

### 4.2 Configurar Docker (Opcional)

Para usar containers ou Open WebUI:

```bash
# Instalar Docker
sudo apt install -y docker.io
sudo systemctl enable --now docker
sudo usermod -aG docker $USER

# Verificar (requer logout/login)
docker --version
```

### 4.3 Configurar Firewall

```bash
# Instalar UFW
sudo apt install -y ufw

# PolÃ­tica padrÃ£o: negar tudo
sudo ufw default deny incoming
sudo ufw default allow outgoing

# Permitir SSH (cuidado para nÃ£o te fechares fora!)
sudo ufw allow ssh

# Permitir acesso local ao Ollama (apenas localhost)
# NÃƒO abrir 11434 para a rede sem autenticaÃ§Ã£o!

# Ativar firewall
sudo ufw enable

# Verificar estado
sudo ufw status verbose
```

---

## 5. InstalaÃ§Ã£o de Modelos

### 5.1 Modelos Essenciais (Recomendado)

```bash
# Llama 3.2 8B - Melhor equilÃ­brio qualidade/velocidade
ollama pull llama3.2:8b

# Mistral 7B - Excelente em portuguÃªs
ollama pull mistral:7b

# Qwen 2.5 Coder - Para programaÃ§Ã£o
ollama pull qwen2.5-coder:7b
```

### 5.2 Modelos Opcionais

```bash
# VersÃ£o mais pequena do Llama (mais rÃ¡pida)
ollama pull llama3.2:3b

# Gemma 2 9B (Google)
ollama pull gemma2:9b

# Modelo de embeddings para RAG
ollama pull nomic-embed-text

# Phi-4 14B (requer CPU offload)
ollama pull phi4:14b
```

### 5.3 Criar Modelo Personalizado PT-PT

```bash
# Criar ficheiro Modelfile
cat > /tmp/Modelfile-pt << 'EOF'
FROM llama3.2:8b

SYSTEM """Ã‰s um assistente de IA especializado em gestÃ£o empresarial, 
desporto e business intelligence. Respondes sempre em PortuguÃªs de Portugal (PT-PT).

Regras de terminologia:
- Usas "base de dados" (nÃ£o "banco de dados")
- Usas "modelaÃ§Ã£o" (nÃ£o "modelagem") 
- Usas "recolha" (nÃ£o "coleta")
- Usas "controlo" (nÃ£o "controle")
- Usas "golos" (nÃ£o "gols")
- Usas "gÃ©nero" (nÃ£o "gÃªnero")
- Usas "treino" (nÃ£o "treinamento")
- Usas "aprendizagem" (nÃ£o "aprendizado")

O teu tom Ã© profissional, acadÃ©mico e adequado para ensino superior."""

PARAMETER temperature 0.7
PARAMETER num_ctx 8192
PARAMETER num_gpu 999
PARAMETER num_thread 16
EOF

# Criar modelo
ollama create llm-local-pt -f /tmp/Modelfile-pt

# Testar
ollama run llm-local-pt "OlÃ¡, confirma que estÃ¡s a funcionar em portuguÃªs de Portugal."
```

---

## 6. VerificaÃ§Ã£o

### 6.1 Verificar Ollama

```bash
# VersÃ£o
ollama --version

# ServiÃ§o ativo
sudo systemctl is-active ollama

# Logs
sudo journalctl -u ollama --no-pager -n 50
```

### 6.2 Verificar Modelos

```bash
# Listar modelos instalados
ollama list

# Deve mostrar algo como:
# NAME                    ID              SIZE      MODIFIED
# llm-local-pt:latest     xxxxxxxx        4.9 GB    2 minutes ago
# llama3.2:8b             xxxxxxxx        4.9 GB    5 minutes ago
# mistral:7b              xxxxxxxx        4.1 GB    5 minutes ago
```

### 6.3 Verificar GPU

```bash
# InformaÃ§Ã£o da GPU
nvidia-smi

# Deve mostrar:
# - Nome da GPU (RTX 2060)
# - Driver Version
# - CUDA Version
# - Processos em execuÃ§Ã£o (incluindo ollama quando ativo)
```

### 6.4 Teste Funcional

```bash
# Teste simples
echo "Explique o RGPD em 3 frases." | ollama run llama3.2:8b

# Teste com timing
time ollama run llama3.2:8b "Qual Ã© a capital de Portugal?"

# Verificar uso de GPU durante execuÃ§Ã£o
# (num terminal separado)
nvidia-smi
```

### 6.5 Benchmark RÃ¡pido

```bash
# Teste de performance
python3 << 'EOF'
import subprocess
import time

model = "llama3.2:8b"
prompt = "Explique o conceito de Business Intelligence em 3 parÃ¡grafos detalhados."

start = time.time()
result = subprocess.run(
    ["ollama", "run", model, prompt],
    capture_output=True,
    text=True
)
end = time.time()

output = result.stdout
num_tokens = len(output.split())
elapsed = end - start
tokens_per_sec = num_tokens / elapsed

print(f"Modelo: {model}")
print(f"Tempo total: {elapsed:.2f}s")
print(f"Tokens gerados: {num_tokens}")
print(f"Tokens/segundo: {tokens_per_sec:.2f}")
print(f"\nPrimeiros 200 caracteres:\n{output[:200]}...")
EOF
```

---

## 7. ResoluÃ§Ã£o de Problemas

### 7.1 Ollama nÃ£o inicia

```bash
# Verificar logs
sudo journalctl -u ollama -n 100 --no-pager

# Verificar portas
sudo netstat -tlnp | grep 11434

# Verificar permissÃµes
ls -la /usr/local/bin/ollama

# Reinstalar se necessÃ¡rio
curl -fsSL https://ollama.com/install.sh | sh
```

### 7.2 GPU nÃ£o Ã© detetada

```bash
# Verificar drivers
nvidia-smi

# Verificar CUDA
nvcc --version

# Verificar permissÃµes do dispositivo
ls -la /dev/nvidia*

# Adicionar utilizador ao grupo video
sudo usermod -aG video $USER
# Fazer logout/login apÃ³s
```

### 7.3 Erro de memÃ³ria (OOM)

```bash
# Reduzir overhead no Ollama
export OLLAMA_GPU_OVERHEAD=1024MB

# Usar modelo mais pequeno
ollama run llama3.2:3b

# Usar apenas CPU
export CUDA_VISIBLE_DEVICES=""
ollama run llama3.2:8b
```

### 7.4 Modelo demora muito a carregar

```bash
# Verificar espaÃ§o em disco
df -h

# Verificar velocidade do disco
sudo hdparm -t /dev/nvme0n1

# Modelos ficam em:
ls -la ~/.ollama/models/

# Limpar modelos nÃ£o usados
ollama prune
```

### 7.5 Problemas de conectividade

```bash
# Ollama funciona offline, mas verificar se serviÃ§o estÃ¡ a escutar
curl http://localhost:11434/api/tags

# Deve retornar lista de modelos em JSON
```

### 7.6 RecuperaÃ§Ã£o de Desastres

```bash
# Backup dos modelos
tar -czf ~/ollama-backup-$(date +%Y%m%d).tar.gz ~/.ollama/

# Restaurar modelos
tar -xzf ~/ollama-backup-YYYYMMDD.tar.gz -C ~/
sudo systemctl restart ollama

# Reinstalar tudo do zero
sudo systemctl stop ollama
rm -rf ~/.ollama
sudo rm -f /usr/local/bin/ollama
curl -fsSL https://ollama.com/install.sh | sh
```

---

## ðŸ“ž Suporte

Se encontrares problemas nÃ£o cobertos neste guia:

1. Consultar logs: `sudo journalctl -u ollama -f`
2. Verificar issues no [GitHub do projeto](https://github.com/[USER]/llm_local_rgpd/issues)
3. Comunidade Ollama: https://github.com/ollama/ollama
4. DocumentaÃ§Ã£o oficial: https://ollama.com/docs

---

**Ãšltima atualizaÃ§Ã£o:** 2025-02-26
