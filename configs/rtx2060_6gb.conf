# Configuração Otimizada para NVIDIA RTX 2060 6GB
# 
# Hardware: MSI GL65 Leopard
# - CPU: Intel i7-10875H (8C/16T)
# - GPU: NVIDIA GeForce RTX 2060 Mobile (6GB VRAM)
# - RAM: 32 GB DDR4
# - Sistema: Ubuntu 24.04 LTS
#
# Esta configuração maximiza a performance mantendo margem
# de segurança para o sistema operativo

[common]
# Número máximo de modelos carregados simultaneamente
# Com 6GB VRAM, apenas 1 modelo de 4-5GB cabe confortavelmente
max_loaded_models = 1

# Tempo para manter modelo em memória após uso (evita reloads constantes)
# 30 minutos = bom equilíbrio entre performance e libertação de recursos
keep_alive = "30m"

# Número de requisições paralelas
# Com 6GB VRAM, 1 pedido de cada vez é o mais seguro
num_parallel = 1

# Tamanho máximo do contexto padrão
# 8192 tokens = bom equilíbrio para a maioria das tarefas
# Pode ser aumentado para 16384 ou 32768 em tarefas específicas
num_ctx = 8192

[gpu]
# Overhead de VRAM reservado para o sistema (MB)
# RTX 2060 6GB = 6144 MB
# Reservar 512MB para o sistema/evitar OOM
# Disponível para modelos: ~5600 MB
gpu_overhead = 512

# Dispositivo CUDA a usar (0 = primeira GPU)
cuda_visible_devices = "0"

# Número de camadas GPU para offload
# -1 = todas as camadas possíveis (automático)
# Valores mais baixos = mais offload para CPU (mais lento)
num_gpu = -1

[cpu]
# Número de threads para processamento CPU
# i7-10875H tem 16 threads - usar todos
num_thread = 16

# Batch size para processamento
# Valores maiores = mais throughput mas mais memória
batch_size = 512

[parameters]
# Temperatura padrão (0.0 = determinístico, 1.0 = criativo)
# Para consultadoria/análise: 0.1-0.3 (mais preciso)
# Para brainstorming: 0.7-0.9 (mais criativo)
temperature = 0.7

# Top-p sampling
# 0.9 = bom equilíbrio entre diversidade e coerência
top_p = 0.9

# Top-k sampling
top_k = 40

# Repetition penalty
# 1.1 = evita repetições sem prejudicar qualidade
repeat_penalty = 1.1

[models.llama3.2-8b]
# Configuração específica para Llama 3.2 8B
# VRAM: ~5GB em Q4_K_M
name = "llama3.2:8b"
num_ctx = 8192
num_gpu = 999  # Todas as camadas em GPU

[models.mistral-7b]
# Configuração específica para Mistral 7B
# VRAM: ~4.5GB em Q4_K_M
# Excelente para Português de Portugal
name = "mistral:7b"
num_ctx = 8192
num_gpu = 999

[models.qwen2.5-coder]
# Configuração específica para Qwen 2.5 Coder
# VRAM: ~4.5GB em Q4_K_M
# Especializado em código Python, R, SQL
name = "qwen2.5-coder:7b"
num_ctx = 16384  # Maior contexto para código
num_gpu = 999

[models.phi-4]
# Configuração específica para Phi-4 14B
# NOTA: Requer CPU offload parcial com 6GB VRAM
# VRAM: ~6.5GB total, GPU: ~5.5GB, RAM: ~1GB
name = "phi4:14b"
num_ctx = 4096  # Contexto menor devido ao tamanho
num_gpu = 25    # Apenas 25 camadas em GPU, resto em CPU

[memory]
# Gestão de memória para sistemas com 32GB RAM

# Mmap: mapear modelo em memória virtual
# true = mais rápido carregamento, usa RAM disponível
use_mmap = true

# Mlock: bloquear modelo em RAM (evita swap)
# false = permite swap se necessário (melhor para sistemas com RAM limitada)
use_mlock = false

# Tamanho do batch para eval
# Maior = mais eficiente mas mais memória
num_batch = 512

[logging]
# Nível de logging
# debug, info, warn, error
level = "info"

# Ficheiro de log
log_file = "/var/log/ollama/ollama.log"

[server]
# Host para binding
# 127.0.0.1 = apenas local (recomendado para segurança)
# 0.0.0.0 = acessível na rede (não recomendado sem autenticação)
host = "127.0.0.1"

# Porta
port = 11434

# CORS origins (para interfaces web)
# Deixar vazio ou definir origens específicas
origins = ["http://localhost", "http://127.0.0.1"]
